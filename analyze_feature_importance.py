# analyze_feature_importance.py
import pandas as pd
from xgboost import XGBClassifier

FEATURE_CSV = "features.csv"
TARGET_COLUMN = "is_drawn"

# ğŸ“¦ è¼‰å…¥ç‰¹å¾µè³‡æ–™
print("ğŸ“¦ è¼‰å…¥ç‰¹å¾µè³‡æ–™...")
df = pd.read_csv(FEATURE_CSV)

# ğŸ§ª åˆ‡åˆ†ç‰¹å¾µèˆ‡æ¨™ç±¤
X = df.drop(columns=["date", "number", TARGET_COLUMN])
y = df[TARGET_COLUMN]

# ğŸ§  è¨“ç·´ XGBoost æ¨¡å‹
print("ğŸ§  è¨“ç·´ XGBoost æ¨¡å‹...")
model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=6,
    eval_metric="logloss",
    random_state=42
)
model.fit(X, y)

# ğŸ“Š é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§ï¼ˆGainï¼‰
print("\nğŸ¯ ç‰¹å¾µé‡è¦æ€§ï¼ˆä¾æ“šè³‡è¨Šå¢ç›Š gainï¼‰æ’åºï¼š")
importance = model.get_booster().get_score(importance_type='gain')
sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)

for rank, (feature, score) in enumerate(sorted_importance, start=1):
    print(f"{rank:>2}. {feature:<20} â†’ gain: {score:.4f}")